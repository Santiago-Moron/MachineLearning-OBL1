{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uiuNIbDf3p6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from skimage import data, color, feature\n",
        "from skimage.transform import rescale, resize\n",
        "\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "\n",
        "from sklearn.feature_extraction.image import PatchExtractor\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, recall_score, precision_score\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.random import set_seed\n",
        "import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-aUsgFUhaDr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "lfw_people = fetch_lfw_people(resize=0.4)\n",
        "labels = lfw_people.target\n",
        "positive_patches = lfw_people.images\n",
        "size = positive_patches[0].shape\n",
        "\n",
        "imgs = ['camera',\n",
        "        'text',\n",
        "        'coins',\n",
        "        'moon',\n",
        "        'page',\n",
        "        'clock',\n",
        "        'immunohistochemistry',\n",
        "        'chelsea',\n",
        "        'coffee',\n",
        "        'hubble_deep_field'\n",
        "        ]\n",
        "\n",
        "images = []\n",
        "for name in imgs:\n",
        "    img = getattr(data, name)()\n",
        "    if len(img.shape) == 3 and img.shape[2] == 3:  # Chequeamos si la imagen es RGB\n",
        "        img = color.rgb2gray(img)\n",
        "    images.append(img)\n",
        "\n",
        "    # Imagenes caseras adicionales\n",
        "for i in range(16):\n",
        "    filename = 'imagenes/'+str(i)+'.jpg'\n",
        "    img = plt.imread(filename)\n",
        "    img = color.rgb2gray(img)\n",
        "    images.append(img)\n",
        "\n",
        "    \n",
        "# Función para extraer porciones de una imagen\n",
        "def extract_patches(img, N, scale=1.0, patch_size=size):\n",
        "    extracted_patch_size = tuple((scale * np.array(patch_size)).astype(int))\n",
        "\n",
        "    extractor = PatchExtractor(patch_size=extracted_patch_size, max_patches=N, random_state=0)\n",
        "    \n",
        "    patches = extractor.transform(img[np.newaxis])\n",
        "    \n",
        "    # Si el factor de escala no es 1, redimensiona cada parche extraído\n",
        "    # al tamaño del parche original\n",
        "    if scale != 1:\n",
        "        patches = np.array([resize(patch, patch_size) for patch in patches])\n",
        "    \n",
        "    return patches\n",
        "\n",
        "# Extraemos las imágenes de fondo\n",
        "negative_patches = np.vstack([extract_patches(im, 250, scale) for im in tqdm(images, desc='Procesando imágenes') for scale in [0.5, 0.75, 1.0, 1.5, 2.0]])\n",
        "negative_patches.shape\n",
        "\n",
        "X_train = np.array([feature.hog(im) for im in tqdm(chain(positive_patches, negative_patches), desc='Construyendo X')])\n",
        "y_train = np.zeros(X_train.shape[0])\n",
        "y_train[:positive_patches.shape[0]] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dividir los datos en conjuntos de entrenamiento y prueba:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creador de plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Graficador:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def graficar(self, y_pred, classifier):\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "        tnr = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
        "        tpr = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
        "        fpr = conf_matrix[0, 1] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
        " \n",
        "        print(f'Accuracy: {accuracy}')\n",
        "        print(f'Precision: {precision}')\n",
        "        print(f'Recall: {recall}')\n",
        "        print(f'True Negative Rate (TNR): {tnr}')\n",
        "        print(f'True Positive Rate (TPR): {tpr}')\n",
        "        print(f'False Positive Rate (FPR): {fpr}')\n",
        " \n",
        "        train_sizes, train_scores, test_scores = learning_curve(classifier, X_train, y_train, cv=5)\n",
        " \n",
        "        plt.figure()\n",
        "        plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label=\"Training score\")  \n",
        "        plt.plot(train_sizes, test_scores.mean(axis=1), 'o-', label=\"Cross-validation score\")\n",
        "        plt.xlabel(\"Training examples\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.title(\"Learning Curve\")\n",
        "        plt.legend(loc=\"best\")\n",
        "        plt.show()\n",
        "\n",
        "graf = Graficador()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gb_classifier = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, random_state=0)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        " \n",
        "predictions = gb_classifier.predict(X_test)\n",
        " \n",
        "graf.graficar(predictions, gb_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ada-Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clasificador débil\n",
        "weak_classifier = DecisionTreeClassifier(max_depth=1)\n",
        "# Buen clasificador en base al débil\n",
        "adaboost_classifier = AdaBoostClassifier(estimator=weak_classifier, n_estimators=20, random_state=0)\n",
        "\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = adaboost_classifier.predict(X_test)\n",
        "\n",
        "graf.graficar(predictions, adaboost_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bagging Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "bagging_classifier = BaggingClassifier(adaboost_classifier, n_estimators=10, random_state=0)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = bagging_classifier.predict(X_test)\n",
        "\n",
        "graf.graficar(predictions, bagging_classifier)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR5ow9MMtUdK"
      },
      "outputs": [],
      "source": [
        "\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=500, max_depth=None,\n",
        "                             max_features=100, n_jobs=-1, random_state=0)\n",
        "t_start = time()\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "time_full_train = time() - t_start\n",
        "auc_full_features = roc_auc_score(y_test, random_forest_classifier.predict_proba(X_test)[:, 1])\n",
        "\n",
        "predicitons = random_forest_classifier.predict(X_test)\n",
        "\n",
        "graf.graficar(predictions, random_forest_classifier)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Crear el clasificador KNN\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "# Entrenar el modelo\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "predictions = knn_classifier.predict(X_test)\n",
        "\n",
        "graf.graficar(predictions, knn_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regresión Logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        },
        "id": "qWK080NEFM_b",
        "outputId": "53f0dfe2-8b70-492d-b903-210f44af62e8"
      },
      "outputs": [],
      "source": [
        "logisticRegression_classifier = LogisticRegression(max_iter=500)\n",
        "logisticRegression_classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = logisticRegression_classifier.predict(X_test)\n",
        "\n",
        "graf.graficar(predictions, logisticRegression_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Red Neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Crear red neuronal\n",
        "def create_model():\n",
        "\n",
        " model = Sequential()\n",
        " model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        " model.add(Dropout(0.5))\n",
        " model.add(Dense(8, activation='relu'))\n",
        " model.add(Dropout(0.5))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        " sgd = SGD(learning_rate=0.05, momentum=0.8)\n",
        " model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        " return model\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        " \n",
        "# Seed para poder reproducir el modelo\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "neuralNetwork_classifier = KerasClassifier(model=create_model, epochs=150, batch_size=128, verbose=0)\n",
        "\n",
        "set_seed(1)\n",
        "logdir = \"logs/neuralNetwork/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "callbacks = [keras.callbacks.TensorBoard(log_dir=logdir), \n",
        "             keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        "\n",
        "\n",
        "#Entrenamiento\n",
        "neuralNetwork_classifier.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=150, callbacks=callbacks)\n",
        "\n",
        "predictions = neuralNetwork_classifier.predict(X_test)\n",
        "\n",
        "\n",
        "# graf.graficar(y_pred, neuralNetwork_classifier)\n",
        "\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "conf_matrix\n",
        "\n",
        "# Guardo Red Neuronal llamado 'model' (opcional)\n",
        "dump(neuralNetwork_classifier, 'modelo.joblib') \n",
        "\n",
        "# Para cargar el modelo en el futuro usar la función 'load'\n",
        "# Se proporciona el nombre del archivo como argumento\n",
        "neuralNetwork_classifier_saved = load('modelo.joblib') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cascade classifier - Modelo Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "class CascadeClassifier:\n",
        "    def __init__(self, classifiers):\n",
        "        self.classifiers = classifiers\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Inicializar predicciones con ceros\n",
        "        predictions = []\n",
        "        for patch in X:\n",
        "            for classifier in self.classifiers:     # Comienza a ejecutar el primer clasificador, corta la ejecución si se predice que no es una cara\n",
        "                pred = True\n",
        "                prediction = classifier.predict([patch])[0] \n",
        "                if (prediction == 0):\n",
        "                    pred = False\n",
        "                    break\n",
        "\n",
        "                \n",
        "            predictions.append(pred)\n",
        "        return np.array(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "classifiers = [adaboost_classifier, logisticRegression_classifier, random_forest_classifier, neuralNetwork_classifier]\n",
        "\n",
        "cascade_classifier = CascadeClassifier(classifiers)\n",
        "\n",
        "y_pred = cascade_classifier.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Métricas de modelo final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "print(f\"AUC Score: {auc_score}\")\n",
        "\n",
        "# Graficar la curva ROC\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (area = {auc_score:.2f})')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset nuevo para pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
        "positive_patches = fetch_olivetti_faces().images\n",
        "\n",
        "# Extraemos los parches de nuevo dataset\n",
        "\n",
        "labels = fetch_olivetti_faces().target\n",
        "positive_patches = fetch_olivetti_faces().images\n",
        "size = positive_patches[0].shape\n",
        "\n",
        "imgs = ['camera',\n",
        "        'text',\n",
        "        'coins',\n",
        "        'moon',\n",
        "        'page',\n",
        "        'clock',\n",
        "        'immunohistochemistry',\n",
        "        'chelsea',\n",
        "        'coffee',\n",
        "        'hubble_deep_field'\n",
        "        ]\n",
        "\n",
        "images = []\n",
        "for name in imgs:\n",
        "    img = getattr(data, name)()\n",
        "    if len(img.shape) == 3 and img.shape[2] == 3:  # Chequeamos si la imagen es RGB\n",
        "        img = color.rgb2gray(img)\n",
        "    images.append(img)\n",
        "\n",
        "    # Imagenes caseras adicionales\n",
        "for i in range(16):\n",
        "    filename = 'imagenes/'+str(i)+'.jpg'\n",
        "    img = plt.imread(filename)\n",
        "    img = color.rgb2gray(img)\n",
        "    images.append(img)\n",
        "\n",
        "    \n",
        "# Función para extraer porciones de una imagen\n",
        "def extract_patches(img, N, scale=1.0, patch_size=size):\n",
        "    # Calcula el tamaño del parche extraído basado en el factor de escala dado\n",
        "    extracted_patch_size = tuple((scale * np.array(patch_size)).astype(int))\n",
        "    \n",
        "    # Inicializa un objeto PatchExtractor con el tamaño de parche calculado,\n",
        "    # el número máximo de parches, y una semilla de estado aleatorio\n",
        "    extractor = PatchExtractor(patch_size=extracted_patch_size, max_patches=N, random_state=0)\n",
        "    \n",
        "    # Extrae parches de la imagen dada\n",
        "    # img[np.newaxis] se utiliza la entrada de PatchExtractor es un conjunto de imágenes\n",
        "    patches = extractor.transform(img[np.newaxis])\n",
        "    \n",
        "    # Si el factor de escala no es 1, redimensiona cada parche extraído\n",
        "    # al tamaño del parche original\n",
        "    if scale != 1:\n",
        "        patches = np.array([resize(patch, patch_size) for patch in patches])\n",
        "    \n",
        "    # Devuelve la lista de parches extraídos (y posiblemente redimensionados)\n",
        "    return patches\n",
        "\n",
        "# Extraemos las imágenes de fondo\n",
        "negative_patches = np.vstack([extract_patches(im, 250, scale) for im in tqdm(images, desc='Procesando imágenes') for scale in [0.5, 0.75, 1.0, 1.5, 2.0]])\n",
        "negative_patches.shape\n",
        "\n",
        "X_test = np.array([feature.hog(im) for im in tqdm(chain(positive_patches, negative_patches), desc='Construyendo X')])\n",
        "y_test = np.zeros(X_train.shape[0])\n",
        "y_test[:positive_patches.shape[0]] = 1\n",
        "\n",
        "\n",
        "y_pred = cascade_classifier.predict(X_test)\n",
        "y_test = y_test[:32900]\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define una función para realizar una ventana deslizante (sliding window) sobre una imagen.\n",
        "def sliding_window(img, \n",
        "                   patch_size=positive_patches[0].shape,  # Define el tamaño del parche (patch) basado en el primer parche positivo por defecto\n",
        "                   istep=2,  # Paso de desplazamiento en la dirección i (verticalmente)\n",
        "                   jstep=2,  # Paso de desplazamiento en la dirección j (horizontalmente)\n",
        "                   scale=1.0):  # Factor de escala para ajustar el tamaño del parche\n",
        "                   \n",
        "    # Calcula las dimensiones Ni y Nj del parche ajustadas por el factor de escala.\n",
        "    Ni, Nj = (int(scale * s) for s in patch_size)\n",
        "    \n",
        "    # Itera a lo largo de la imagen en la dirección i\n",
        "    for i in range(0, img.shape[0] - Ni, istep):\n",
        "        # Itera a lo largo de la imagen en la dirección j\n",
        "        for j in range(0, img.shape[1] - Ni, jstep):\n",
        "            \n",
        "            # Extrae el parche de la imagen usando las coordenadas actuales i, j.\n",
        "            patch = img[i:i + Ni, j:j + Nj]\n",
        "            \n",
        "            # Si el factor de escala es diferente de 1, redimensiona el parche al tamaño original del parche.\n",
        "            if scale != 1:\n",
        "                patch = resize(patch, patch_size)\n",
        "            \n",
        "            # Usa yield para devolver las coordenadas actuales y el parche. \n",
        "            # Esto convierte la función en un generador.\n",
        "            yield (i, j), patch\n",
        "\n",
        "\n",
        "def non_max_suppression(indices, Ni, Nj, overlapThresh):\n",
        "    # Si no hay rectángulos, regresar una lista vacía\n",
        "    if len(indices) == 0:\n",
        "        return []\n",
        "\n",
        "    # Si las cajas son enteros, convertir a flotantes\n",
        "    if indices.dtype.kind == \"i\":\n",
        "        indices = indices.astype(\"float\")\n",
        "\n",
        "    # Inicializar la lista de índices seleccionados\n",
        "    pick = []\n",
        "\n",
        "    # Tomar las coordenadas de los cuadros\n",
        "    x1 = np.array([indices[i,0] for i in range(indices.shape[0])])\n",
        "    y1 = np.array([indices[i,1] for i in range(indices.shape[0])])\n",
        "    x2 = np.array([indices[i,0]+Ni for i in range(indices.shape[0])])\n",
        "    y2 = np.array([indices[i,1]+Nj for i in range(indices.shape[0])])\n",
        "\n",
        "    # Calcula el área de los cuadros y ordena los cuadros\n",
        "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    idxs = np.argsort(y2)\n",
        "\n",
        "    # Mientras todavía hay índices en la lista de índices\n",
        "    while len(idxs) > 0:\n",
        "        # Toma el último índice de la lista y agrega el índice a la lista de seleccionados\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        "\n",
        "        # Encontrar las coordenadas (x, y) más grandes para el inicio de la caja y las coordenadas (x, y) más pequeñas para el final de la caja\n",
        "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
        "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
        "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
        "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
        "\n",
        "        # Calcula el ancho y alto de la caja\n",
        "        w = np.maximum(0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0, yy2 - yy1 + 1)\n",
        "\n",
        "        # Calcula la proporción de superposición\n",
        "        overlap = (w * h) / area[idxs[:last]]\n",
        "\n",
        "        # Elimina todos los índices del índice de lista que tienen una proporción de superposición mayor que el umbral proporcionado\n",
        "        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlapThresh)[0])))\n",
        "\n",
        "    # Devuelve solo las cajas seleccionadas\n",
        "    return indices[pick].astype(\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargo imagen de prueba\n",
        "\n",
        "test_image_path = \"imagenes/Central.jpg\"\n",
        "test_image = plt.imread(test_image_path)\n",
        "test_image = color.rgb2gray(test_image)\n",
        "test_image = rescale(test_image,0.25)\n",
        "test_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizamos la imagen\n",
        "# Buscamos la escala de los rostros\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(test_image, cmap='gray')\n",
        "\n",
        "scale = 0.3\n",
        "Ni, Nj = (int(scale * s) for s in positive_patches[0].shape)\n",
        "\n",
        "ax.add_patch(plt.Rectangle((0, 0), Nj, Ni, edgecolor='red', alpha=0.3, lw=2, facecolor='none'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utiliza la función de ventana deslizante en una imagen de prueba.\n",
        "# zip(*...) toma las tuplas generadas y las descompone en índices y parches.\n",
        "indices, patches = zip(*sliding_window(test_image, scale=scale))\n",
        "\n",
        "# Calcula las características HOG para cada parche y las almacena en un array.\n",
        "patches_hog = np.array([feature.hog(patch) for patch in patches])\n",
        "\n",
        "# Muestra la forma del array de características HOG.\n",
        "patches_hog.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "labels = cascade_classifier.predict(patches_hog).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizamos las detecciones\n",
        "\n",
        "Ni, Nj = (int(scale*s) for s in positive_patches[0].shape)\n",
        "indices = np.array(indices)\n",
        "detecciones = indices[labels == 1]\n",
        "detecciones = non_max_suppression(np.array(detecciones),Ni,Nj, 0.1)\n",
        "\n",
        "# Algoritmo que elimina las detecciones que se encuentran dentro de otras detecciones\n",
        "\n",
        "\n",
        "# Visualizamos las detecciones\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(test_image, cmap='gray')\n",
        "ax.axis('off')\n",
        "\n",
        "for i, j in detecciones:\n",
        "    ax.add_patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='red',\n",
        "                               alpha=0.3, lw=2, facecolor='none'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
